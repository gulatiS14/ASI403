<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Project Title</title>
  <link rel="icon" href="./favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/themes/light.css" />
  <script type="module"
    src="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/shoelace-autoloader.js"></script>

  <!-- You can add what components you want to include here  -->
  <link rel="stylesheet" href="../index.css">
  <link rel="stylesheet" href="../components/scroll-to-top/scroll-to-top.css">
  <script src="../components/scroll-to-top/scroll-to-top.js"></script>
  <script src="../components/image/image-component.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.2.1/dist/mermaid.min.js"></script>
  <script src="../components/diagram/svg-pan-zoom.min.js"></script>
  <script type="module" src="../components/video/video.js"></script>
  <!--  -->
</head>

<body>
  <div class="content">
    <sl-button href="../">Back</sl-button>
    
    <h1>2. Vision Sub-system</h1>

    <p> The following report will be a continuation from Semester 1, please read through the archived part link below to recap:</p>

    <sl-tree-item >
        <a href="../Sem_1/vision/">Vision (Sem 1 Archived)</a>
      </sl-tree-item>

      <div id="Value-Prop-and-Design-Statement">
        <h2>2.1 Sub-system specific Value Proposition and Design Statement</h2>
        <p>
            Originally, the vision subsystem was focused on providing geological context to help select the best site for soil sampling in search of signs of life. However, with the shift in priorities, the value proposition now also emphasizes establishing a robust spectroscopy system and a fully functioning GUI. This updated approach not only supports informed site selection and comprehensive scientific documentation but also streamlines operator tasks by integrating advanced spectroscopic capabilities with an intuitive user interface.
        </p>
    
        <p><b>The 2 main value propositions that were identified are now:</b></p>
        
        <p>
            1. A geological imaging and spectroscopy system that transforms the rover's sampling capabilities by providing crucial contextual information and scientific analysis tools. This enables informed site selection, comprehensive scientific documentation, and efficient mission execution.
        </p>
            
        <p>
            2. By integrating panoramic views, high-resolution close-ups, stratigraphic profiling, and spectroscopic analysis, the system ensures each sample collected has maximum scientific value. It fulfills URC requirements while providing operators with detailed insights necessary for detecting potential biosignatures.
        </p>
            
        <p><b>From these value propositions, the following design statements can be obtained to design an integrated system that can:</b></p>
        
        <p><b>Design Statement (1): </b>Combine panoramic vision with high-resolution imaging to support each phase of the scientific investigation process, such as providing accurate images for stratigraphic profiles.</p>
    
        <p><b>Design Statement (2): </b>Enable the team to document geological context and soil details while precisely identifying optimal sampling locations. By automating scale calculations and location documentation, the design allows operators to focus on scientific analysis rather than technical operation, maximizing the scientific return from each collected sample.</p> 
    
        <p><b>Design Statement (3): </b>Create from scratch an easy-to-use GUI for science operators. This interface will integrate with the spectroscopy system to simplify operational tasks, enhance mission efficiency, and fulfill URC requirements.</p> 
    
        <p>
            In this semester, the main focus has been to create a functioning science module by laying a strong foundation through comprehensive documentation, applying industry best practices, and employing rigorous scientific methods to achieve optimal results.
        </p>
    </div>
    
    
    
    <div id="Concept-Development">
        <h2>2.2 Concept Development</h2>
        <p>
            During the concept development phase, numerous innovative approaches were conceptualized to build robust pipelines supporting the system. The primary focus was on integrating components such as video streaming, GUI design, and sensor interfacing to optimize performance and ensure scalability. This exploratory phase laid the foundation for the tailored solutions implemented in the final design.
        </p>
          

        <h3>2.2.1 Different Ways to Setup GUI</h3>
            <p>
            For the rover's mission, achieving low-latency access to GPIO controls is essential. In this context, the design must enable real-time stereo camera streaming (using the ZED2i) while ensuring responsive control via a dedicated GUI. The challenge lies in balancing hardware constraints—using a Jetson for processing and streaming, and a Raspberry Pi (RPi) for direct GPIO management—with overall network reliability and user experience.
            </p>

            <p>
            The following architectural options were examined:
            </p>

            <ul>
            <li>
                <strong>Current Approach (Optimal):</strong> The GUI and server run on the RPi while the Jetson processes and streams the ZED2i data via UDP. This method offers direct GPIO access through the RPi, ensuring rapid response times and maintaining a safe, isolated network environment for critical controls.
            </li>
            <li>
                <strong>Alternative Approaches:</strong> Other options considered include hosting the GUI on the Jetson with a reverse camera stream from the RPi, accessing the RPi directly via SSH, or consolidating all functionalities on the Jetson. However, these alternatives present challenges such as increased latency, complex remote GPIO control, and potential single points of failure.
            </li>
            </ul>

            <p>
            A critical evaluation of these alternatives highlights that the current approach is most effective. Direct control via the RPi minimizes latency and avoids the pitfalls associated with SSH overhead or dual streaming complications. Furthermore, by decoupling the processing and control functionalities, the system leverages each hardware component's strengths—GPU-accelerated streaming on the Jetson and real-time GPIO management on the RPi.
            </p>

            <image-component 
            tag="image" 
            source="../assets/Vision/gui_morph.png"
            subtitle="Table 2.: Weighted concept selection matrix for choosing the GUI architecture">
            </image-component>

            <h4>Challenges with Each Approach</h4>

                <p>
                Each architectural alternative presents specific limitations in the context of rover deployment. Direct SSH access to the RPi compromises latency and system robustness, especially under unstable network conditions. Hosting the GUI on Jetson requires remote GPIO control over the network, introducing latency and increasing failure points. A fully Jetson-hosted solution faces physical wiring constraints and possible voltage mismatches for GPIO control. These limitations emphasize the need for a hybrid approach that balances computational efficiency, real-time GPIO responsiveness, and operational resilience.
                </p>

            <h4>Why Current Approach is Optimal</h4>

                <p>
                The current design leverages the strengths of both processing units — the Jetson efficiently handles GPU-accelerated camera streaming, while the RPi offers direct, low-latency access to GPIOs without network dependencies. This architecture provides network resilience through static IP communication and isolates security risks by avoiding SSH exposure. Furthermore, it supports future scalability without requiring a complete system redesign, making it the most suitable choice for rover operations in dynamic and uncertain environments.
                </p>

        <h3>2.2.2 Choosing the Correct Video Streaming Method</h3>

            
                <p>
                  For the rover’s mission, it is essential to stream high-quality video from the ZED2i camera to the Raspberry Pi for real-time monitoring and control. Several streaming options were considered to meet the demands of low latency and system reliability. These options include raw UDP sockets, a GStreamer pipeline, the native ZED2i SDK API, and TCP-based streaming. Each method offers distinct advantages and drawbacks, making it crucial to evaluate them against our hardware constraints and performance needs.
                </p>
                
                <h4>Evaluation of Streaming Methods</h4>
                <p>
                  <strong>Raw UDP Sockets:</strong> This method involves direct frame-by-frame transmission via UDP packets. It offers low-level control and minimal overhead, making it ideal for achieving the low latency required for real-time video feedback and GPIO control.
                </p>
                <p>
                  <strong>GStreamer Pipeline:</strong> Although GStreamer provides hardware-accelerated streaming through RTP/RTSP plugins, it relies on GPU codecs and complex pipeline configurations. The lack of stable support for these plugins on the Raspberry Pi OS Lite creates potential issues, such as increased latency and system instability.
                </p>
                <p>
                  <strong>ZED2i SDK API:</strong> The native API is optimized for GPU processing; however, it can monopolize GPU resources on the Jetson. This resource-intensive method might starve other critical computer vision tasks, limiting overall system performance.
                </p>
                <p>
                  <strong>TCP Sockets:</strong> While TCP ensures reliable data delivery, its retransmission mechanisms introduce significant delays. In a real-time control environment, these delays could adversely affect the synchronization between video streaming and actuator commands.
                </p>
                
                <image-component 
                    tag="image" 
                    source="../assets/Vision/stream_morph.png"
                    subtitle="Table 2.: Different lenses and their specifications">
                </image-component>
                
                <h4>Why Raw UDP Was Chosen</h4>
                <p>
                  Among the evaluated methods, raw UDP emerged as the optimal choice. Its connectionless nature minimizes overhead and eliminates the need for GPU-dependent codecs, ensuring that the Jetson’s processing power is reserved for critical computer vision tasks. Additionally, UDP's customizable error handling allows for better management of packet loss, maintaining synchronization between video feedback and the rover's GPIO-controlled actuators.
                </p>
                
                    
                

        <h3>2.2.3 Choosing the Right Zoom Lens</h3>
            <p>
            Choosing the right zoom lens was a critical decision for our rover’s imaging system. One of the primary challenges involved positioning the zoom lens due to mechanical constraints—specifically, a single linear actuator attached to the auger left limited options for mounting the camera close to the actuator. As a result, an alternative approach was proposed: fixing the zoom lens directly into the chassis. This solution required the use of a telephoto lens to achieve the necessary optical zoom while securely attaching the lens, ensuring that both the camera and lens are safely housed in the harsh, rugged environments encountered during URC missions.
            </p>

            <image-component 
                    tag="image" 
                    source="../assets/Vision/lens_on_Chasis.jpg"
                    subtitle="Figure 2.: 3D model of lens on the rover">
            </image-component>

            <p>
            Two competing lens options were evaluated: the 16mm telephoto lens and the 8–50mm Arducam zoom lens. The 16mm telephoto lens is fully compatible with the Raspberry Pi HQ Camera’s standard C-mount system and a 1/2.3-inch sensor, offering a narrow field of view and moderate magnification ideal for detailed close-up imaging. Its manual focus mechanism allows for precise adjustments—an essential feature for scientific documentation—while its compact and robust design minimizes the risk of damage in extreme conditions. In contrast, the 8–50mm Arducam zoom lens, although versatile with its adjustable focal length, presents added mechanical complexity and bulk, which could compromise durability and performance in our application.
            </p>

            <image-component 
            tag="image" 
            source="../assets/Vision/lens_morph.png"
            subtitle="Table 2.: Different lenses and their specifications">
            </image-component>

            <p>
            Ultimately, the 16mm telephoto lens was selected as the preferred solution, as it is specifically optimized for our proprietary camera system and meets the operational requirements of our use case. The descision was based on component availability and compatibility with the HQ Camera Sensor. More testing with the USB Camera will be shown in later part of the report.
            </p>
            

    <div id="Prototyping">
        <h2>2.3 Prototyping</h2>

        <p>
            The vision subsystem's prototyping focused primarily on software development, with initial testing conducted on alternative hardware platforms to verify functionality. All software prototyping was implemented using Docker containers in a development environment that mirrors the rover's operational system. This approach allowed us to validate key imaging functions before final hardware integration.
        </p>


        <h3>GUI for Teleoperator Control during Science Missions</h3>

        <h4>User Journey Map</h4>
        <ul>
          <li><strong>Initial Setup:</strong> The teleoperator powers on the rover and accesses the GUI dashboard.</li>
          <li><strong>Live Monitoring:</strong> 
            <ul>
              <li><strong>RPi Camera Feed:</strong> Offers a zoomed-in view to inspect detailed geological context and pinpoint excavation sites.</li>
              <li><strong>ZED2i Stereo Feed:</strong> Provides left and right views for a comprehensive observation of the surroundings.</li>
            </ul>
          </li>
          <li><strong>Control Actions:</strong> 
            <ul>
              <li><strong>Actuator Control:</strong> Adjusts the auger’s vertical movement to collect soil samples.</li>
              <li><strong>Stepper Motor Control:</strong> Rotates the carousel to correctly position the imaging system.</li>
              <li><strong>Stitch Function:</strong> Captures images from both the left and right cameras and merges them into a panoramic photo.</li>
            </ul>
          </li>
          <li><strong>Mission Execution:</strong> The teleoperator continuously monitors the rover’s status and adjusts controls in real time to ensure successful data acquisition and sample collection.</li>
        </ul>
        
        <h4>System Functionality Overview</h4>

            <p>
            This GUI is designed to provide the teleoperator with real-time control and feedback during the science mission. Each component of the system has a clear role in supporting the operator’s decision-making process, especially when selecting and sampling geological sites.
            </p>

            <h4>Camera Feeds</h4>

            <p>
            <strong>RPi Camera Feed:</strong> 
            The Raspberry Pi camera stream is managed using the <code>PiCamera</code> library, which captures frames and serves them via a Flask-based HTTP server. This feed offers a zoomed-in perspective to help the operator examine finer geological details and decide where to collect soil samples.
            </p>

            <p>
            <strong>ZED2i Stereo Camera Feed:</strong>
            The ZED2i camera provides dual (left and right) video feeds, allowing the operator to maintain a broader view of the rover’s surroundings. By comparing both streams, the teleoperator can better gauge distances, identify obstacles, and plan maneuvers for optimal sample collection.
            </p>

            <h4>Control Buttons and System Actions</h4>

            <image-component 
            tag="image" 
            source="../assets/Vision/gui_controls.png"
            subtitle="Figure 2.: Different buttons on the GUI and their functionality">
            </image-component>
            <p>
            Multiple buttons in the GUI control essential functions of the rover’s science payload:
            </p>
            <ul>
            <li><strong>Actuator Up/Down:</strong> Moves the auger vertically to collect soil samples.</li>
            <li><strong>Stepper Motor Left/Right:</strong> Rotates the carousel for positioning the imaging or sampling system.</li>
            <li><strong>Stitch Function:</strong> Captures and merges images from the left and right cameras into a single panoramic view, providing a more comprehensive picture of the site.</li>
            </ul>

            <p>
            To maintain responsiveness under concurrent operations, each button’s backend function uses <code>threading.Lock()</code>. This prevents race conditions or overlapping commands. For instance:
            </p>

            <pre><code>lock = threading.Lock()

            def move_actuator_up():
                with lock:
                    # Code to send command to move actuator up
            </code></pre>

            <p>
            This ensures only one operation executes at a time, preventing conflicts and keeping the interface responsive even when multiple controls are used in quick succession.
            </p>

            <h4>Image Stitching Feature</h4>

            <p>
            The "Stitch" button is a key function for geological documentation. When activated, it triggers image captures from both the left and right ZED2i feeds. The system then uses OpenCV’s stitching library to merge these images into a panoramic view. This panorama is invaluable for assessing the broader geological context and planning further excavation or sample analysis.
            </p>
            
            <p>
                A video demonstration showcasing the fully functioning GUI has also been attached below. This video highlights the real-time camera streaming, responsive button controls, and the overall user interaction with the system during a simulated science mission scenario.
              </p>
              

            <div class="video-container">
                <video-component tag="rick" source="https://drive.google.com/file/d/1VlVqh7j5nhwUEJ6LJOwl-pocPSoyA64T/preview" width="640" height="480" allow="autoplay"
                subtitle="Video: Functioning GUI with ZED2i and RPi Camera Feeds">
                </video-component>
            </div>

        
        <!-- <p>
            Github sites for the following:
            <li>
                <a href="https://github.com/gulatiS14/Image-Stitching-OpenCV-Demo">Image Stitching</a>
            </li>
            <li>
                <a href="https://github.com/gulatiS14/ROS2-image-stitcher">ROS2 Image Stitcher</a>
            </li>
        </p> -->

        <h4>How does the GUI work?</h4>

        <p>
            The GUI is built on a Flask web server, which serves as the central hub for all interactions between the teleoperator and the rover. When the operator clicks a button in the web interface, a POST request is sent to the corresponding Flask endpoint. This request triggers specific functions on the Raspberry Pi that control various hardware components.
        </p>

        <image-component 
            tag="image" 
            source="../assets/Vision/button_flow.png"
            subtitle="Figure 2.: Flow of pressing the button on the website">
        </image-component>
          
          <p>
            For instance, when a command to move the actuator is received, the Flask route calls a function that utilizes the gpiozero library to control the GPIO pins connected to the actuator. The code employs a threading lock to ensure that only one movement command is processed at a time, thereby preventing any overlapping or conflicting operations. This dedicated function then signals the motor to move either up or down, depending on the command, and monitors the operation until completion or until a stop command is issued.
          </p>
          
          <p>
            In parallel, the GUI also manages live video feeds from both the RPi camera and the ZED2i camera. The Flask server streams these feeds through separate endpoints that deliver MJPEG data to the browser, ensuring that the teleoperator receives continuous real-time visual feedback. This coordinated workflow, combining responsive control commands and synchronized video streaming, enables seamless operation of the rover during science missions.
          </p>

          <p>
            The GUI is designed to be user-friendly and intuitive, allowing the teleoperator to focus on the mission objectives without being bogged down by complex controls. The layout is organized to provide quick access to essential functions, with clear labels and visual indicators for each control. This design philosophy ensures that the operator can efficiently manage the rover's operations while maintaining situational awareness of the geological context being captured.
          </p>
          

    

        <h3>Testbench for Spectrometer</h3>

            <p>
            The spectrofluorometer system was initially developed by a separate team in the previous semester. This device was designed specifically for the purpose of detecting signs of life by analysing the fluorescence characteristics of a sample. As part of the current phase of development, there was a critical requirement to understand the working principles of this spectrometer system and integrate it effectively within the vision subsystem of the overall project.
            </p>

            <image-component 
            tag="image" 
            source="../assets/Vision/spec.png"
            subtitle="Figure 2.: The Spectrometer system">
            </image-component>

            <p>
            In order to facilitate this integration and to enable robust testing of the spectrometer’s functionality, a dedicated spectroscopy testbench was designed and developed. This testbench serves as a controlled environment where multiple spectroscopy-related tests can be conducted systematically, ensuring repeatability and scientific accuracy.
            </p>

            <p>
            To accommodate the various tests required, a rotating carousel mechanism was conceptualized and implemented. This carousel was specifically designed to hold five cuvettes, each allocated for a different test sample. The cuvettes were positioned equidistantly along the circular platform to optimize the spatial arrangement and maintain balance during rotation. At the center of the carousel, a fixed slot was allocated for the light source. This central positioning ensures uniform illumination of the cuvettes, thereby improving the reliability and consistency of the spectral measurements.
            </p>

            <image-component 
            tag="image" 
            source="../assets/Vision/rot_car.png"
            subtitle="Figure 2.: Rotating Carousell for the spectroscopy testbench">
            </image-component>

            <p>
            In addition to the spatial design, noise reduction and isolation of the cuvettes were addressed. Guard walls were incorporated around each cuvette slot to minimize ambient light interference and prevent external noise from affecting the spectral readings. This consideration was crucial to uphold the scientific validity of the data collected from each test.
            </p>

            <p>
            For actuation, a stepper motor was installed underneath the carousel platform. The motor was programmed to rotate the carousel precisely, enabling the selection of the desired cuvette for testing. The motor control was integrated with the main software, allowing dynamic rotation based on the current testing requirements. This automation greatly enhanced the efficiency of the testing process, reducing manual intervention and improving accuracy.
            </p>
            
            <div class="video-container">
                <video-component tag="rick" source="https://drive.google.com/file/d/1x3EDK_idseH5jX24LS-t725eoNFRK_y7/preview" width="640" height="480" allow="autoplay"
                subtitle="Video: Carousell for the spectroscopy testbench, rotating to the next cuvette">
                </video-component>
            </div>

            <p>
            Overall, the spectroscopy testbench provides a reliable and modular setup to perform multiple spectroscopic tests in a controlled manner. Its design emphasizes precision, spatial efficiency, and scientific robustness, making it an essential component for validating the spectrometer’s capabilities before field deployment.
            </p>


        <h3>2.3.2 Deep Learning for Soil Classification</h3>

        <p>
            To streamline our sample site selection process at the URC, we have developed a machine learning-based soil classification system. Our approach uses convolutional neural networks (CNNs) to analyze imagery from our dedicated "close-up" camera, automatically classifying soil types into three categories based on grain size: gravel (greater than 2mm), sand (less than 1/16mm - 2mm), and silt/clay (1/16mm) can be seen in Figure 2.9 (Katwyk, 2023). Currently, the model is trained on a limited public dataset of 75 geological images with data augmentation techniques including rotation, scaling, and brightness variations to improve generalization. While our initial dataset is small, this automated grain size analysis serves as a proof-of-concept to complement our other scientific instruments.
        </p>

        
        <image-component 
        tag="image" 
        source="../assets/vis_prop_cnn.png"
        subtitle="Figure 2.9: Types of soil Classification">
        </image-component>

        <p>
            Our soil classification CNN model processes 256x256 RGB images through a series of convolutional blocks. Each block consists of a Conv2D layer with 3x3 kernels, followed by ReLU activation and 2x2 max pooling, progressively extracting features from the input image. After the convolutional blocks, the features are flattened and passed through dense layers with ReLU activation. The final layer uses softmax activation to output classification probabilities for three soil types: gravel, sand, and silt/clay, can be seen in flowchart in Figure 2.10.
        </p>

        <image-component 
        tag="image" 
        source="../assets/vis_prop_cnn_1.png"
        subtitle="Figure 2.10: Flowchart of the CNN model">
        </image-component>
        

        <p>
            While relatively simple in architecture, this model demonstrates promising initial results can see at Figure 2.11 , though we acknowledge the need for additional training data to improve robustness in field conditions.
        </p>


        <image-component 
        tag="image" 
        source="../assets/vis_prop_cnn_2.png"
        subtitle="Figure 2.11: Testing of the model">
        </image-component>

        <p>
            Analysis of the confusion matrix as seen in Figure 2.12, reveals potential biases and limitations in our current soil classification model. The matrix shows that class 1 (likely sand) has the strongest predictive accuracy with 5 correct predictions, while classes 0 and 2 (likely gravel and silt) show some concerning misclassifications between each other. This pattern suggests a bias in our model, primarily stemming from our limited dataset. Despite data augmentation techniques, the model struggles to clearly differentiate between certain soil types, particularly in edge cases. These results emphasize the critical need to expand our training dataset with more diverse soil samples to improve the model's generalization capabilities.
        </p>

        <image-component 
        tag="image" 
        source="../assets/vis_prop_cnn_3.png"
        subtitle="Figure 2.12: Confusion Matrix for our Model">
        </image-component>

        <a href="https://github.com/gulatiS14/Soil_Classification">Github link for the Soil Classification</a>
        
    </div>

    <div id="Testing-Analysis">
        <h2>2.4 Testing and Analysis</h2>

        <h3>2.4.1 Testing of Close-Up Shots</h3>

        <p>
        As part of the University Rover Challenge science mission requirements, we need to implement a reliable close-up imaging system to document potential sampling sites with high detail and accurate scaling. We are evaluating two distinct approaches for detailed geological documentation: the Raspberry Pi Camera Module 3 NoIR, featuring a 12-megapixel sensor with 4608 × 2592 pixel resolution and a versatile focus range from 10 cm to infinity, and a Digital Microscope USB camera offering 0.3 megapixels (680 × 480 pixels) with a specialized close-up focus range of 15 mm to 40 mm.
        </p>

        <image-component 
        tag="image" 
        source="../assets/vis_test_tab_1.png"
        subtitle="Table 2.5: Table showing the two different approaches">
        </image-component>

        <p>
            As seen in Table 2.5, these options represent two different methodologies for gathering detailed geological evidence - the Raspberry Pi camera asking "Can we get the information from an overhead camera?" while the digital microscope addresses "Do we need to zoom in to the soil?" This analysis will evaluate how these contrasting approaches serve our need to capture well-focused images at the sampling sites, and help the mechanical team to house the camera in the rover.
        </p>

        <p>
            In order to compare the two cameras, a few tests were conducted under lab conditions. The tests will focus on three key aspects: spatial resolution, image noise analysis, and no-reference image quality assessment.
        </p>
        
        <h3>Spatial Resolution</h3>
        <p>
            To evaluate the cameras' ability to capture fine geological details, we will measure their spatial resolution by calculating pixels per millimeter using a standardized calibration target. For the Raspberry Pi Camera's 4608 × 2592 resolution and the Digital Microscope's 680 × 480 resolution, we'll determine their effective resolution at their respective working distances (Gonzalez & Woods, 2018). This measurement is crucial for ensuring we can adequately document small-scale geological features in the field.
        </p>

        <h3>Image Noise Analysis</h3>
        <p>
            Image noise assessment will be conducted under controlled lighting conditions. We will measure the signal-to-noise ratio (SNR) for both cameras, with particular attention to their performance in varying light conditions that simulate field operations (Yang & El Gamal, 1999). The Raspberry Pi Camera's larger sensor size suggests potentially better noise handling, but this needs to be verified through systematic testing.
        </p>

        <h3>No-Reference Image Quality Assessment</h3>
        <p>
            To quantitatively compare image quality without relying on reference images, we will employ the Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) algorithm. BRISQUE offers advantages over other no-reference metrics as it operates in the spatial domain, providing faster computation and better correlation with human perception of image quality (Mittal et al., 2012). This method will provide objective quality scores for images captured by both cameras, helping us evaluate their performance in documenting geological features under real-world conditions.
        </p>

        <p>
            In order to compare the difference between the two methodologies, a standardised test was conducted where they are aiming to focus on word 'I2C'. The results can be seen in the Table 2.6 below:
        </p>

        <image-component 
        tag="image" 
        source="../assets/vis_test_tab_2.png"
        subtitle="Table 2.6: Table comparing the scores of different methodolgies with the comparing image of the two approaches">
        </image-component>

        <p>
            After getting the quatifiable results for each category, an analysis was done to see which camera is better. The results can be seen in the Table 2.7 below:
        </p>

        <image-component 
        tag="image" 
        source="../assets/vis_test_tab_3.png"
        subtitle="Table 2.7: Table analysing the scores from Table and analysing the different methodologies">
        </image-component>

        <p>
            Our testing has revealed a key challenge: while the RaspberryPi Camera Module 3 NoIR excels in color accuracy and noise handling, and the USB Digital Microscope provides superior spatial resolution, neither solution fully optimizes both critical parameters. This suggests the need to explore alternative solutions that could better balance these requirements.
        </p>

        <h3>
            Potential Alternative: Raspberry Pi HQ Camera with Arducam  Zoom Lens
        </h3>

        <image-component 
        tag="image" 
        source="../assets/vis_test_fig_1.png"
        subtitle="Figure 2.13: Raspberry Pi HQ Camera with zoom lens">
        </image-component>
        
        <p>
            The combination of the Raspberry Pi HQ Camera (IMX477 sensor) with the Arducam 8-50mm C-Mount Zoom Lens presents an optimal solution that addresses the limitations of our tested cameras as seen in Figure 2.13 (Arducam, n.d.). This configuration offers several key advantages such as:
        </p>

        <ul>
            <li>Higher base resolution (12.3MP Sony IMX477 sensor)</li>
            <li>Superior zoom range (8-50mm) offering greater versatility than both tested options</li>
            <li>Larger sensor size (7.9mm diagonal) for better light capture</li>
            <li>Professional-grade optics through the C-Mount lens system</li>
            <li>Adjustable focus range that exceeds both current options</li>
            <li>Better low-light performance due to larger pixel size (1.55μm)</li>
        </ul>

        <p>
            Key advantages of this solution include:
        </p>

        <ul>
            <li>Variable zoom capability allowing both wide context shots and detailed close-ups</li>
            <li>Industrial-grade build quality suitable for field operations</li>
            <li>Mechanical zoom and focus controls for precise adjustments</li>
            <li>Compatible with our existing Raspberry Pi infrastructure</li>
        </ul>
        <p>
            Given these capabilities, we recommend proceeding with the development of a prototype using the Raspberry Pi HQ Camera with Arducam zoom lens configuration. This combination promises to deliver both the spatial resolution and color accuracy required for our geological documentation while providing superior operational flexibility through its adjustable zoom range.
        </p>       


    </div>
    
    <div id="Deliverables-Short-Coming">
        <h2>2.5 Deliverables and Short-comings</h2>

        <h3>2.5.1 Deliverables Satisfied</h3>
        
        <ul>
            <li><strong>Panoramic Imaging System:</strong>
                <ul>
                    <li>Successfully implemented image stitching pipeline</li>
                    <li>Creates high-quality panoramic views from multiple stereo camera images</li>
                </ul>
            </li>
            <li><strong>Geological Classification:</strong>
                <ul>
                    <li>Developed CNN model for terrain analysis</li>
                    <li>Trained on comprehensive dataset of sedimentary materials</li>
                    <li>Capable of identifying sand, gravel, and silt compositions</li>
                </ul>
            </li>
            <li><strong>Scale Measurement System:</strong>
                <ul>
                    <li>Initiated development of photogrammetric scaling method</li>
                    <li>Utilizes stereo camera depth sensing capabilities</li>
                    <li>Combines FOV and depth data for real-world dimension calculations</li>
                </ul>
            </li>
        </ul>

        <h3>2.5.2 Short-comings</h3>

        <ul>
            <li><strong>Close-up Imaging System:</strong>
                <ul>
                    <li>Current camera options inadequate for detailed sample documentation</li>
                    <li>Insufficient magnification and working distance capabilities</li>
                    <li>Need to integrate dedicated macro photography solution</li>
                </ul>
            </li>
            <li><strong>Scale Measurement Limitations:</strong>
                <ul>
                    <li>Photogrammetric method unreliable for distant geological features</li>
                    <li>Stereo camera depth sensing range insufficient for far objects</li>
                    <li>Need to implement hybrid approach combining GNSS data</li>
                </ul>
            </li>
            <li><strong>Model Validation:</strong>
                <ul>
                    <li>CNN model requires testing with actual camera system images</li>
                    <li>Need to verify accuracy under competition lighting conditions</li>
                    <li>Performance validation needed for specific camera resolutions and perspectives</li>
                </ul>
            </li>
        </ul>

    </div>

    <div id="Future-Work">
        <h2>2.6 Future Work</h2>

        <image-component 
        tag="image" 
        source="../assets/vis_soft_arch.png"
        subtitle="Figure 2.14: System architecture for vision subsystem">
        </image-component>
        
        <p>
            The vision subsystem's architecture as seen in Figure 2.14, integrates multiple components that require comprehensive testing in outdoor environments. Critical integration testing will focus on the data flow between the Zed 2i stereo camera and Jetson Orin for panoramic imaging, alongside the RPI HQ Camera with zoom lens connecting to the Raspberry Pi 5 for detailed sample documentation.
        </p>

        <p>
            Future work will concentrate on developing a unified software pipeline that efficiently manages data flow between these components while minimizing latency. We plan to conduct extensive field testing at locations with similar geological features to the competition site, particularly focusing on the system's performance under various lighting conditions, dust exposure, and temperature variations. Additionally, we will implement automated calibration procedures for both cameras to maintain accurate depth perception and scale measurements. 
        </p>

        <p>
            The testing protocol will include validating the CNN model's performance with real-time image processing, ensuring the system can maintain consistent performance during extended operation periods, and developing fault tolerance mechanisms for potential communication failures between components
        </p>

    </div>

    <sl-button href="../mechanical/">Next Subsystem</sl-button>

    <sl-button class="scroll-to-top" variant="primary" size="medium" circle onclick="scrollToTop()">
      <sl-icon name="arrow-up" label="Settings"></sl-icon>
    </sl-button>

    <script src="../components/diagram/diagram.js"></script>
    </div>
</body>

</html>