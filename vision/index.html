<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Project Title</title>
  <link rel="icon" href="./favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/themes/light.css" />
  <script type="module"
    src="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/shoelace-autoloader.js"></script>

  <!-- You can add what components you want to include here  -->
  <link rel="stylesheet" href="../index.css">
  <link rel="stylesheet" href="../components/scroll-to-top/scroll-to-top.css">
  <script src="../components/scroll-to-top/scroll-to-top.js"></script>
  <script src="../components/image/image-component.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11.2.1/dist/mermaid.min.js"></script>
  <script src="../components/diagram/svg-pan-zoom.min.js"></script>
  <script type="module" src="../components/video/video.js"></script>
  <!--  -->
</head>

<body>
  <div class="content">
    <sl-button href="../">Back</sl-button>
    
    <h1>2. Vision Sub-system</h1>

    <div id="Value-Prop-and-Design-Statement">
        <h2>2.1 Sub-system specific Value Proposition and Design statement</h2>
        <p> The vision subsystem's main use case is to provide the geological context around as well as, to find the best site for rover to sample soil for finding signs of life.</p>
    
        <p><b> The 2 main value propositions that were identified are as follows:</b></p>
        
        <p> A geological imaging system that transforms the rover's sampling capabilities by providing crucial contextual information that enables informed site selection, comprehensive scientific documentation, and efficient mission execution.</p>
            
        <p> By integrating panoramic views, high-resolution close-ups, and stratigraphic profiling capabilities, the system ensures each sample collected has maximum scientific value for detecting potential biosignatures in Mars-analog environments and fullfil the URC requirements.</p>
            
        
    
        <p><b>From these value propositions, the following design statements can be obtained to design an integrated camera system that can:</b></p>
        
        <p><b>Design statement (1): </b>It combines panoramic vision with targeted close-up capabilities to support each phase of the scientific investigation process.</p>
    
        <p><b>Design statement (2): </b>The system will enable team to document geological context, provide accurate images for stratigraphic profiles, and precisely identify optimal sampling locations.</p>
            
        <p><b>Design statement (3): </b>By automating scale calculations and location documentation, the design allows operators to focus on scientific analysis rather than technical operation, maximizing the scientific return from each collected sample.</p>
    </div>
    
    <div id="Concept-Development">
        <h2>2.2 Concept Development</h2>
        <p>
          The geological complexity of the Mars Desert Research Station (MDRS) environment, with its "horizontal layers" and diverse "regolith terrain systems" [THE_REGOLITH_GEOLOGY_OF_THE_MDRS_STUDY_AREA.pdf], demands sophisticated imaging solutions. The science team's design process explores three critical functions for the rover's vision system: wide-angle photography for contextual documentation, close-up imaging for detailed sample analysis, and methods for identifying optimal sampling locations.
        </p>
        
        <p>These functions are discussed in detail in the following subsections, and Table 1 shows the morphological chart for the overall subsystem: </p>

        <image-component 
        tag="image" 
        source="../assets/vis_morph_1.jpg"
        subtitle="Table 1: Morphological chart for vision component selection">
        </image-component>

        <h3>2.2.1 For the function to take “wide shots”</h3>

        <p>
          The team evaluates three potential approaches for wide-angle photography: a stereo camera system that provides depth perception and wide field of view capabilities, a 360° panoramic camera for complete site documentation, or a dual monocular setup. The integration with ROS 2 becomes particularly advantageous here, as it "leverages its strengths in inter-process communication and software modularity" [Team Mountaineers SAR 2024.pdf], allowing efficient handling of high-bandwidth image data and seamless integration with OpenCV for image processing.
        </p>
        
        <p>In order to represent each category, a market study was conducted, and the following options were chosen:</p>

        <ol>
            <li>
                <p><strong>Stereo Camera:</strong></p> 
                <p><strong>ZED 2i Stereo Camera:</strong> A professional-grade stereo vision camera with 4K capability and built-in depth perception.</p>
            </li>
            <li>
                <p><strong>360 Degree Camera:</strong></p> 
                <p><strong>Insta360 X3:</strong> A 360-degree camera capable of capturing full panoramic views in a single shot.</p>
            </li>
            <li>
                <p><strong>Wide Monocular Camera:</strong></p> 
                <p><strong>Yealink UVC30:</strong> A high-quality 4K monocular USB camera with good low-light performance.</p>
            </li>
        </ol>
        
        <image-component 
        tag="image" 
        source="../assets/vis_cam_sel_1.png"
        subtitle="Figure 1: Showing each camera selected for each group">
        </image-component>

        <p>
            The team will evaluate these options based on weighted concept selection matrix where criteria such as image quality, field of view and depth perception most importantly, as it is necessary to meet the minimum reuiremnets. This can be seen in the Table 2 below:
        </p>

        <image-component 
        tag="image" 
        source="../assets/vis_con_sel_1.png"
        subtitle="Table 2: Weighted concept selection matrix for choosing the component for wide eyed functionality">
        </image-component>

        <p>
            From the above component matrix, the ZED 2i stereo camera is selected for our URC rover implementation. Other than the benefits in camera quality, it connects via USB Type-C (USB 3.0) to our NVIDIA Jetson platform, enabling real-time spatial computing with depth sensing from 0.3m to 20m. The ZED 2i is well-suited for the harsh desert environment with its IP66 rating, providing complete protection against dust and strong water jets. Its operating temperature range of -10°C to +45°C and robust aluminium construction ensure reliable performance in the Mars Desert Research Station conditions. Compatible with the Jetson platform and requiring only ≥2GB GPU memory, it offers an optimal balance of performance and integration capabilities for our rover's requirements.
        </p>

        <h3>2.2.2 For the function to take “close-up shots”</h3>

        <p>
          For close-up documentation, the team is considering a dedicated microscopic camera for maximum magnification, a high-resolution camera with electronic zoom capabilities, or a hybrid autofocus system. This capability is crucial as teams must "take a close up, well-focused, high-resolution picture with some indication of scale" at sampling sites.
        </p>
        
        <p>In order to represent each category, a market study was conducted to look for the most feasible and popular options to represent each category:</p>
        
        <ol>
            <li>
                <p><strong>Microscopic Camera:</strong></p> 
                <p><strong>Off-the-shelf Digital Microscope:</strong> Most popular choice among hobbyists and makers.</p>
            </li>
            <li>
                <p><strong>High Resolution with electronic zoom:</strong></p> 
                <p><strong>RPI Camera:</strong> Industry standard for robotics.</p>
            </li>
            <li>
                <p><strong>Hybrid Autofocus Camera:</strong></p> 
                <p><strong>Olympus Tough TG-6:</strong> Professional-grade macro photography capabilities.</p>
            </li>
        </ol>

        <p>Further analysis is done using a weighted component selection matrix, to determine the suitable choice, which can be seen in the Table 3 below:</p>

        <image-component 
        tag="image" 
        source="../assets/vis_con_sel_2.png"
        subtitle="Table 3: Weighted concept selection matrix for choosing the component for close up functionality">
        </image-component>

        <p>While both the Digital Microscope Camera and RPi Camera with E-Zoom show promising characteristics (tied at 7.80/10), with the Hybrid Autofocus Camera scoring lower (6.45/10), further testing is required to understand which is the better choice for the mission especially to find the correct sampling location. This testing will likely reveal practical differences not apparent in theoretical analysis and will ensure the selected system meets all mission requirements effectively.</p>

        <h3>2.2.3 For the function of identifying sampling locations</h3>

        <p>
            Finally, for identifying optimal soil sampling locations, the team evaluated three approaches: manual intervention based on operator expertise, deep learning algorithms for automated feature detection, and hyperspectral imaging for compositional analysis. Site selection is particularly critical at MDRS where soil conditions vary significantly - from cracking clays with high salinity to non-cracking clays that exhibit different moisture retention properties (Clarke, 2003).
        </p>
        
        <image-component 
        tag="image" 
        source="../assets/vis_con_sel_3.png"
        subtitle="Table 4: Weighted concept selection matrix for choosing the methodology of finding site">
        </image-component>

        <p>
            The concept selection analysis for sample site identification indicates that a hybrid approach combining manual intervention with deep learning support is optimal for the URC science mission requirements. Manual operation leverages human geological expertise for final decision-making, ensuring scientific rigor in site selection and stratigraphic analysis similar to approaches used in Mars missions (Clarke, 2003; NASA Jet Propulsion Laboratory, 2018). This is augmented by a deep learning system that assists operators by highlighting potential features of interest, comparable to AI-assisted geological feature detection systems deployed on Mars rovers (Neveu & Hays, 2018; Li et al., 2023). The manual component scored 7.95/10 in our weighted analysis, with deep learning scoring 7.75/10, reflecting their complementary strengths. While hyperspectral imaging could provide valuable compositional data (Olszewska & Napora, 2020), it was excluded due to hardware constraints and mission requirements being adequately met with the selected hybrid approach. This combination maximizes accuracy while maintaining operational efficiency, aligning with established Mars exploration methodologies (Clarke, 2003; Neveu & Hays, 2018).
        </p>
        
    </div>

    <div id="Prototyping">
        <h2>2.3 Prototyping</h2>

        <p>
            The vision subsystem's prototyping focused primarily on software development, with initial testing conducted on alternative hardware platforms to verify functionality. All software prototyping was implemented using Docker containers in a development environment that mirrors the rover's operational system. This approach allowed us to validate key imaging functions before final hardware integration.
        </p>

        <h3>2.3.1 Image Stitching for Panaroma</h3>

        <p>For the University Rover Challenge, we developed a real-time panoramic imaging system to document sites and get the better understanding of the geological context.</p>
        
        <p>Our system uses the Scale-Invariant Feature Transform (SIFT) algorithm for image processing. SIFT effectively handles the varied scales and orientations found in natural terrain. The process converts images to grayscale, detects key features, and filters matches using a ratio test to ensure accuracy under different lighting conditions.</p>
        
        <p>We implemented RANSAC (Random Sample Consensus) to calculate image alignment through homography. This approach effectively manages outliers and uneven terrain typical in competition environments, ensuring geometrically accurate panoramas for scientific documentation.</p>
        
        <image-component 
        tag="image" 
        source="../assets/vis_prop_is.png"
        subtitle="Figure : Diagram explaining how the image stitching works">
        </image-component>

        <pre class = "mermaid" style="border: 1px solid black">
            flowchart TD
            A[Load Images] --> B[Convert to Grayscale]

            subgraph Critical[Critical Steps]
            direction TB
            B --> C[Extract Features using SIFT]
            C --> D[Match Features using BFMatcher]
            D --> E[Calculate Homography Transform]
            end

            subgraph Important[Important Steps]
            direction TB
            E --> F[Filter Matches using Ratio Test]
            F --> G[Blend Images using warpPerspective]
            end

            G --> H[Save Panorama]

            style Critical fill:#ffecec,stroke:#ff4444
            style Important fill:#e6f3ff,stroke:#4477ff
            style A fill:#f9f9f9,stroke:#333
            style H fill:#f9f9f9,stroke:#333
        </pre>
        <p style="font-size: 1rem; font-style: italic; text-align: center;">
            Figure : Flowchart of image stitching algorithm 
        </p>

        <p>
            The system, built using OpenCV and Python, integrates smoothly with our Robot Operating System (ROS) framework, enabling quick development and deployment.
        </p>
        
        <p>
            Initial testing was conducted using two cameras mounted 10cm apart to evaluate real-time panoramic imaging capabilities. The cameras were interfaced through ROS2 Humble running in a Docker container, utilizing OpenCV for image stitching. While the SURF feature detection algorithm successfully identified matching key points between adjacent frames, the stitching process, was of poor quality which can be seen in the video due to the poor hardware and stitching being done at 30 frames per second. 
        </p>
        <video-component tag="rick" source="https://drive.google.com/file/d/1X3-ifnGLrvob-dnLmB0QrIdE9RL2adAW/preview" width="640" height="480" allow="autoplay"
          subtitle="Video: ROS2 image stitching Demo">
        </video-component>

        <p>
            A ROS2-based state machine is developed for automated panorama capture using a single ZED 2i stereo camera. The system subscribes to three key topics: '/camera/image_raw' for visual data and '/gnss/fix' for positional information. The state machine executes a predetermined sequence where the rover rotates 90 degrees between captures, taking four overlapping stereo images to complete a 360-degree view. Each image is automatically annotated with elevation and coordinates from the GNSS receiver, and an automatically calculated scale bar derived from the stereo camera's depth data. This approach proved more reliable and computationally efficient and will be tested in the future.
        </p>

        <pre class = "mermaid" style="border: 1px solid black">
            stateDiagram-v2
            [*] --> IDLE
            IDLE --> CAPTURING_INITIAL: Start Capture / Subscribe RGB & GNSS
            CAPTURING_INITIAL --> ROTATING: Capture Success / Store Image & GNSS
            CAPTURING_INITIAL --> ERROR: Capture Fail / Publish Status
            ROTATING --> CAPTURING_ROTATED: Rotation Complete / Cmd_vel Stop
            ROTATING --> ERROR: Rotation Fail / Publish Status
            CAPTURING_ROTATED --> ROTATING: More Angles Needed / Cmd_vel Rotate 90°
            CAPTURING_ROTATED --> STITCHING: All Images Captured / Begin Processing
            CAPTURING_ROTATED --> ERROR: Capture Fail / Publish Status
            STITCHING --> COMPLETED: Stitch Success / Publish Panorama & GNSS
            STITCHING --> ERROR: Stitch Fail / Publish Status
            ERROR --> IDLE: Reset
            COMPLETED --> IDLE: Reset

            state IDLE {
                [*] --> Waiting
                Waiting : Subscribes to RGB and GNSS Topics
            }

            state ROTATING {
                [*] --> Moving
                Moving : Publishes cmd_vel for Rotation
                Moving : Monitors Rotation Completion
                Moving : Publishes Status
            }

            state COMPLETED {
                [*] --> Publishing
                Publishing : Publishes Panorama Image
                Publishing : Publishes Success Status
                Publishing : Includes GNSS Data
            }

            note right of IDLE
                Subscribes:
                - /zed2i/zed_node/rgb/image_rect_color
                - /zed2i/zed_node/gnss/fix
            end note

            note right of ROTATING
                Publishes:
                - /cmd_vel (90° rotation)
                - /panorama/status
            end note

            note right of COMPLETED
                Publishes:
                - /panorama/image
                - /panorama/status
                With GNSS overlay
            end note  
        </pre>
        <p style="font-size: 1rem; font-style: italic; text-align: center;">
            Figure : State diagram of the rover's vision system process.
        </p>

        
    </div>

    <div id="Testing-Analysis">
        <h2>2.4 Testing and Analysis</h2>
    </div>
    
    <div id="Deliverables-Short-Coming">
        <h2>2.5 Deliverables and Short-comings</h2>
    </div>

    <div id="Future-Work">
        <h2>2.6 Future Work</h2>
    </div>

    <sl-button href="../mechanical/">Next Subsystem</sl-button>

    <sl-button class="scroll-to-top" variant="primary" size="medium" circle onclick="scrollToTop()">
      <sl-icon name="arrow-up" label="Settings"></sl-icon>
    </sl-button>

    <script src="../components/diagram/diagram.js"></script>
    </div>
</body>

</html>